{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# for PC USERS:\n",
    "# sys.path.insert(1, '.\\Software\\Library_Files')\n",
    "\n",
    "# for MAC USERS:\n",
    "# sys.path.insert(1, './Software/Library_Files')\n",
    "\n",
    "# DO NOT ALTER THE FOLLOWING LINES\n",
    "\n",
    "import CurveFitting as cf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data_set = pd.read_csv('length_weight_mercury_data.csv')\n",
    "numer_data = data_set.to_numpy()\n",
    "\n",
    "ages = numer_data[1:,0].astype(float)\n",
    "measured_ages = ages[~np.isnan(ages)]\n",
    "\n",
    "lengths = numer_data[1:,1].astype(float)\n",
    "measured_lengths = lengths[~np.isnan(ages)]\n",
    "\n",
    "weights = numer_data[1:,2].astype(float)\n",
    "\n",
    "mercury = numer_data[1:,3].astype(float)\n",
    "measured_mercury = mercury[~np.isnan(ages)]\n",
    "\n",
    "dmat0 = np.zeros((np.size(measured_ages), 2))\n",
    "dmat0[:, 0] = measured_ages\n",
    "dmat0[:, 1] = 2.54*measured_lengths\n",
    "\n",
    "dmat1 = np.zeros((np.size(lengths), 2))\n",
    "dmat1[:, 0] = np.log(lengths)\n",
    "dmat1[:, 1] = np.log(weights)\n",
    "\n",
    "dmat2 = np.zeros((np.size(measured_ages), 2))\n",
    "dmat2[:, 0] = measured_ages\n",
    "dmat2[:, 1] = measured_mercury"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Fast Introduction to Fitting Models to Data\n",
    "\n",
    "The world is full of data.  I hate it, but it's true.  The data in particular we will be concerned with is data in which we think there is a functional connection.  By this I mean, suppose we have the $N$ points $\\left\\{(x_{j},y_{j})\\right\\}_{j=1}^{N}$.  This is our raw data.  Now we suppose that there is some parameter dependent function $f(x,a,b)$ such that \n",
    "\n",
    "$$\n",
    "y_{j} = f(x_{j},a,b).\n",
    "$$\n",
    "\n",
    "Here, the function $f(x,a,b)$ is our **model**.  It's an assumption, perhaps well justified, perhaps not, that reprsents some belief we have about the structure of our data, which in this case is that each $y_{j}$ depends $x_{j}$ in essentially the same way.  Different models we might choose are:\n",
    "\n",
    "* Linear, i.e. $f(x,a,b) = ax + b$\n",
    "* Exponential, $f(x,a,b) = ae^{bx}$ though then $\\ln(f) = \\ln(a) + bx$, so really this is a linear model in disguise\n",
    "* Nonlinear... well this could be almost anything else.  \n",
    "\n",
    "Now the question is, for a given model choice, how do we find parameter values $a$ and $b$ which best work with the data set we've been given?  Well, a natural way (though not the only one) is to require that $(a,b)$ be chosen to minimize the quantity $E(a,b)$ where\n",
    "\n",
    "$$\n",
    "E(a,b) = \\sum_{j=1}^{N}\\left(y_{j} - f(x_{j},a,b) \\right)^{2}\n",
    "$$\n",
    "\n",
    "To find minima, we need to solve the equations\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial a}E(a,b) = -2\\sum_{j=1}^{N}\\left(y_{j} - f(x_{j},a,b) \\right)\\frac{\\partial}{\\partial a}f(x_{j},a,b) = 0,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial}{\\partial b}E(a,b) = -2\\sum_{j=1}^{N}\\left(y_{j} - f(x_{j},a,b) \\right)\\frac{\\partial}{\\partial b}f(x_{j},a,b) = 0,\n",
    "$$\n",
    "\n",
    "In the case of linear models, these equations become somewhat straightforward.\n",
    "\n",
    "**Problem**: For a linear model $f(x,a,b)=ax+b$, find the solutions $a$ and $b$ which minimize $E(a,b)$ using the equations above.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the simple case of linear models, we need computers to solve the above problem.  Thus the use of CurveFitting in the import statement at the beginning of this notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mercury Accumulation in Lake Trout\n",
    "\n",
    "* Mercury (denoted as Hg on the Periodic Table), a heavy metal, is a dangerous neurotoxin that is very difficult to remove from the body.\n",
    "* It concentrates in the tissues of fish, particularly large predatory fish such as Northern Pike, Lake Trout, Bass, and Walleye.\n",
    "* The primary sources of mercury in the Great Lakes region: Runoff from mining, incinerators that burn waste \n",
    "* Bacteria converts mercury into the highly soluable methyl mercury, enters fish by passing over gills, or when large fish consume small ones.  \n",
    "* Mercury accumulates in tissue, and so we can anticipate that older, larger fish have higher degrees of mercury in their bodies.  \n",
    "\n",
    "![](mercury_in_fish.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling Mercury in Fish\n",
    "\n",
    "A way to model how mercury accumulates in fish is the following:\n",
    "\n",
    "* We use the **von Bertalanffy** equation to model the length, $L(t)$, of a fish over time.  This model is given by the differential equation\n",
    "$$\n",
    "\\frac{dL}{dt} = b(L_{\\ast} - L), ~ L(0) = 0.\n",
    "$$\n",
    "* Use an **allometric** model for the weight, $W(t)$, of a fish relative to its length, $L(t)$.  This takes the form \n",
    "$$\n",
    "W = aL^{k}, ~ k \\in \\mathbb{N}\n",
    "$$\n",
    "or after taking logarithms \n",
    "$$\n",
    "\\ln W(t) = \\ln a + k \\ln L(t).\n",
    "$$\n",
    "* We suppose that the amount of mercury in a given fish, $H(t)$, changes in time in direct proportion to its weight, $W(t)$, so that \n",
    "$$\n",
    "\\frac{dH}{dt} = \\kappa W = \\kappa a L^{k}.\n",
    "$$\n",
    "\n",
    "Now, the idea here is, I give you various data, and using said data, we find the parameters $(b,L_{\\ast},a,k,\\kappa)$ which best fit our model to the data using the optimization approach described above in this notebook.  As an aside, this relatively simple model illustrates how contemporary modeling is done more broadly.  We can, and do, use more complicated models and fitting approaches, but at its core, everything done at the cutting edge of research today boils down to the process we explore in this notebook.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: Solve the von Bertalnaffy equation to find $L(t)$ in terms of the parameters $b$ and $L_{\\ast}$.  Show work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: Solve for $H(t)$ in terms of all of the parameters in the problem.  Note, the binomial theorem will be helpful to deal with that $L^{k}$ term.  Using your results, write $H(t) = \\kappa g(t, b, L_{\\ast}, a, k)$.  What is $g(t,b,L_{\\ast},a,k)$?  Show work.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting to Data\n",
    "\n",
    "So now, let's get an idea of what our data set looks like.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_set.to_markdown(showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So huh... someone wasn't able to measure the age of the fish in all cases.  This is why we have 'nan' for some of the ages.  Well that's unsatisfying, but that's how it usually goes in real world situations.  We don't always get to know everything we'd like to.  Also, note that (ppm) stands for 'parts per million', which corresponds to 1 milligram of mercury per 1 kg gram of body weight.  \n",
    "\n",
    "Let's now get some visuals to help us better understand what we have in front of us.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsize = 8\n",
    "fig, axes = plt.subplots(ncols=1, nrows=3, figsize=(wsize, wsize))\n",
    "\n",
    "axes[0].scatter(measured_ages, measured_lengths, c='k')\n",
    "axes[0].set_xlabel('Age (yrs)')\n",
    "axes[0].set_ylabel('Length (in)')\n",
    "\n",
    "axes[1].scatter(lengths, weights, c='k')\n",
    "axes[1].set_xlabel('Length (in)')\n",
    "axes[1].set_ylabel('Weight (g)')\n",
    "\n",
    "axes[2].scatter(weights, mercury, c='k')\n",
    "axes[2].set_xlabel('Weight (in)')\n",
    "axes[2].set_ylabel('Hg (ppm)')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting Length to Age using the von Bertalanffy model.  \n",
    "\n",
    "So, you should have shown that \n",
    "$$\n",
    "L(t) = L_{\\ast}\\left(1 - e^{-bt}\\right)\n",
    "$$\n",
    "Now as we can see, this is a nonlinear model which maps age, i.e. the variable $t$, into lengths, or $L(t)$.  To figure out the best parameter fits for $L_{\\ast}$ and $b$, we use the bit of code below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = cf.vonbertalfit(dmat0)\n",
    "Lstar = params[0]/2.54\n",
    "b = params[1]\n",
    "print(\"The parameters were found to be:\")\n",
    "print([Lstar, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the results from this code and your prior experience in the group work, fill in the missing pieces of code below and generate a plot of your fit relative to the data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = lambda t: # np.exp() is going to be your friend here\n",
    "tvals = np.linspace(np.min(measured_ages), np.max(measured_ages), 100)\n",
    "plt.plot(tvals, fun(tvals), color='r', label = 'Fit')\n",
    "plt.scatter(measured_ages, measured_lengths, c='k')\n",
    "plt.xlabel('Age (yrs)')\n",
    "plt.ylabel('Length (in)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: Based on your parameter fit and model, what is the longest you would ever expect these fish to get?  Does that seem reasonable?  Use comparisons to the data to justify your explanation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting Length to Weight via the Allometric Model\n",
    "\n",
    "So, in this case, we just have the logarithmic/linear fit\n",
    "\n",
    "$$\n",
    "\\ln W = \\ln(a) + k \\ln(L)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aparams = cf.allometricfit(dmat1)\n",
    "loga = aparams[0]\n",
    "k = aparams[1]\n",
    "print(\"The parameters were found to be:\")\n",
    "print([loga, k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the results from this code and your prior experience in the group work, fill in the missing pieces of code below and generate a plot of your fit relative to the data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = # I'm seeing if you're paying attention with this one\n",
    "fun = lambda L: # again, in Python we raise things to powers with **, as in x**2\n",
    "tvals = np.linspace(np.min(lengths), np.max(lengths), 100)\n",
    "plt.plot(tvals, fun(tvals), color='r', label = 'Fit')\n",
    "plt.scatter(lengths, weights, c='k')\n",
    "plt.xlabel('Length (in)')\n",
    "plt.ylabel('Weight (g)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: Now, we said that $k\\in \\mathbb{N}$, so we need to round the result we just found.  Should you round up or down, and why?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: Using the von Bertalnaffy model for the length, what is the heaviest you would expect these fish to get?  How does this compare to the available data?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting Mercury Levels to Weight\n",
    "\n",
    "We now have everything in place to fit mercury levels in fish to their size, measured by weight.  At this point, we see that we have only one parameter left to fit, and that is $\\kappa$.  Now, we have one more wrinkle.  The data we have for mercury is in units of parts per million, which is really a concentration, $c(t)$, by weight.  That is to say, if we want $H(t_{j})$, we need to find it from   \n",
    "\n",
    "$$\n",
    "H(t_{j}) = c(t_{j})W(t_{j}).\n",
    "$$\n",
    "\n",
    "Again, $c(t_{j})$ is milligrams/kg, so we need to be thoughtful about dimensional analysis here.  \n",
    "\n",
    "Using the results you derived above, we see that we can formulate this fitting problem as \n",
    "\n",
    "$$\n",
    "\\min_{\\kappa} \\sum_{j=1}^{N}\\left(c(t_{j})W(t_{j}) - \\kappa g(t_{j},b, L_{\\ast}, a, k)  \\right)^{2}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: Using differentiation with respect to $\\kappa$, derive a forumla for $\\kappa$ which minimizes the above fitting model.  Show work.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above and the bit of code below, we can readily find $\\kappa$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kval = # your integer value of k you finally decided upon.\n",
    "fit_terms = cf.mercuryfit(dmat2, Lstar, b, a, kval) \n",
    "kappa = fit_terms[0]\n",
    "print(\"The parameter was found to be:\")\n",
    "print(kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all of this now in place, we can fit mercury concentration to time and produce the following curve.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvals = np.linspace(np.min(measured_ages), np.max(measured_ages), 100)\n",
    "fitcurve = kappa*cf.gfun(tvals,Lstar,b,a,kval)/cf.wfun(tvals,Lstar,b,a,kval)\n",
    "plt.plot(tvals, fitcurve, color='r', label = 'Fit')\n",
    "plt.scatter(measured_ages, measured_mercury, c='k')\n",
    "plt.xlabel('Age (yrs)')\n",
    "plt.ylabel('Mercury (ppm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: Comment upon the quality of your model overall.  \n",
    "* Where does it seem to do well?  \n",
    "* Where does it seem poor?  \n",
    "* What do you think would possibly improve it?  \n",
    "* Based on your results, how might you advise a community near a mercury filled lake to approach eating fish from said lake?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
